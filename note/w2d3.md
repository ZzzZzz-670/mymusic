### CSS
f0f0f0 R(00-ff) G(00-ff) B(00-ff) 

### JavaScript
无法用html写算法
JS弥补不足，与Python相似的解释型语言（实时）

### URL
http(协议scheme) :// 
www.example.com(域名Domain Name，个人电脑有IP地址，域名相当于名字->IP的映射，非单射) : 
80(端口port，对应程序，这样同样的域名对应不同的端口，对应不同操作信息到不同程序的绑定) 
/path/to/myfile.html.html(文件路径Path to the file)

- URL（Uniform Resource Locator，统一资源定位符）用于唯一标识互联网上的资源。
- URL 的基本结构：
  ```
  协议://域名:端口/路径?查询参数#片段
  ```
  - 协议（scheme）：如 http、https、ftp，指定访问方式。
  - 域名（Domain Name）：如 www.example.com，也可以是 IP 地址。
  - 端口（port）：可选，http 默认 80，https 默认 443。
  - 路径（path）：服务器上资源的具体位置，如 /index.html。
  - 查询参数（query）：可选，?key=value&key2=value2，传递额外信息。
  - 片段（fragment）：可选，#section1，定位到页面内某部分。

- 示例：
  ```
  https://www.example.com:8080/path/to/page.html?user=123#top
  ```
  - https：协议
  - www.example.com：域名
  - 8080：端口
  - /path/to/page.html：路径
  - ?user=123：查询参数
  - #top：片段

- 注意事项：
  - URL 中的特殊字符需要进行编码（如空格编码为 %20）。
  - 查询参数常用于 GET 请求传递数据。
  - 片段不会被发送到服务器，仅用于客户端定位页面内容。


### HTTP协议
**相当于写封信，请求-代理*n-相应**

- HTTP（HyperText Transfer Protocol，超文本传输协议）是Web通信的基础协议，用于客户端（如浏览器）与服务器之间的数据传输。
- 特点：
  - 无连接：每次请求/响应后连接即断开。
  - 无状态：服务器不会记录客户端的状态，每次请求独立。
  - 基于请求-响应模式：客户端发送请求，服务器返回响应。

- HTTP请求格式：
  ```
  请求行
  请求头
  空行
  请求体（可选）
  ```
  例如：
  ```
  GET (方法)/index.html HTTP/1.1
  Host: www.example.com
  User-Agent: Mozilla/5.0

  ```

- 常见请求方法：
  - GET：获取资源（常用于查询）。
  - POST：提交数据（如表单）。
  - PUT：更新资源。
  - DELETE：删除资源。

- HTTP响应格式：
  ```
  状态行
  响应头
  空行
  响应体
  ```
  例如：
  ```
  HTTP/1.1 200 OK
  Content-Type: text/html（返回内容）

  <html>...</html>
  ```

- 常见状态码：
  - 200：请求成功
  - 301/302：重定向
  - 404：未找到资源
  - 500：服务器内部错误

- HTTPS：HTTP的安全版本，数据加密传输，防止窃听和篡改。

cookie保存了辨认身份的编码，以保证用户的连续使用

### Web应用的呈现
* 前端：可交互部分
* 后端：存储数据、计算等（控制层，模型层，数据库）
* 设计模式：
服务端渲染：服务端整体渲染好整体以HTML返回给客户端
客户端渲染： 交互逻辑，需要时客户端向服务端请求数据进行页面渲染
前后端分离是大趋势，前后端通过约定的API进行协作，不需要代码层面的直接交互。


### JSON

- JSON（JavaScript Object Notation，JavaScript对象表示法）是一种轻量级的数据交换格式，常用于前后端数据传输。
- 特点：
  - 结构清晰，易于阅读和编写。
  - 语言无关，几乎所有编程语言都支持解析和生成 JSON。
  - 以键值对（key-value）形式组织数据，支持嵌套对象和数组。

- 基本语法：
  - 数据以对象（{}）或数组（[]）形式表示。
  - 键必须用双引号包裹，值可以是字符串、数字、布尔值、数组、对象或 null。
  - 例如：
    ```json
    {
      "name": "张三",
      "age": 20,
      "isStudent": true,
      "courses": ["数学", "英语"],
      "info": {
        "school": "某大学",
        "year": 2024
      }
    }
    ```

- 常见用途：
  - 前后端数据交互（如 AJAX、RESTful API）。
  - 配置文件（如 package.json）。
  - 数据存储与传输。

- 注意事项：
  - JSON 不支持注释。
  - 键必须用双引号，不能用单引号。
  - 末尾不能有多余的逗号

### 实操
ctrl shift I

### requests
requests
params传参可以自动转义，可以同时传多个值
我们需要手动添加user-agent，否则网站会发现你是通过python在访问。

- requests 是 Python 中常用的第三方 HTTP 库，用于发送网络请求。
- 安装方法：
  ```
  pip install requests
  ```

- 常用方法及示例：

  1. **GET 请求**（获取数据，带参数）：
     ```python
     import requests
     params = {'key1': 'value1', 'key2': 'value2'}
     response = requests.get('https://httpbin.org/get', params=params)
     print(response.status_code)    # 状态码
     print(response.url)            # 实际请求的URL
     print(response.text)           # 返回内容（字符串）
     print(response.json())         # 返回内容（字典，若为JSON）
     ```

  2. **POST 请求**（提交数据）：
     ```python
     import requests
     data = {'username': 'test', 'password': '123456'}
     response = requests.post('https://httpbin.org/post', data=data)
     print(response.text)
     ```

  3. **自定义请求头（如 User-Agent）**：
     ```python
     headers = {'User-Agent': 'Mozilla/5.0'}
     response = requests.get('https://httpbin.org/get', headers=headers)
     print(response.text)
     ```

  4. **发送 JSON 数据**：
     ```python
     import requests
     json_data = {'name': '张三', 'age': 20}
     response = requests.post('https://httpbin.org/post', json=json_data)
     print(response.json())
     ```

  5. **文件上传**：
     ```python
     files = {'file': open('test.txt', 'rb')}
     response = requests.post('https://httpbin.org/post', files=files)
     print(response.text)
     ```

  6. **保存响应内容到文件**（如下载图片）：
     ```python
     response = requests.get('https://www.example.com/image.png')
     with open('image.png', 'wb') as f:
         f.write(response.content)
     ```

- 常用属性和方法：
  - `response.status_code`：响应状态码
  - `response.text`：响应内容（字符串）
  - `response.content`：响应内容（二进制）
  - `response.json()`：将响应内容解析为 JSON（字典）
  - `response.headers`：响应头信息（字典）

- 注意事项：
  - 建议加上 `headers`（如 User-Agent），防止被网站识别为爬虫。
  - 网络请求可能失败，实际开发中建议加异常处理（如 try-except）。
  - params 传参会自动转义，适合 GET 请求；POST 请求用 data 或 json 参数

### BeautifulSoup4

find_all(name,attrs,resuese,balabala)

find_all(href=re.compile("elsie"),id'link1')

- BeautifulSoup4 是 Python 中常用的网页解析库，主要用于从 HTML 或 XML 文档中提取数据，常用于爬虫开发。
- 安装方法：
  ```
  pip install beautifulsoup4
  ```
- 常见用法：
  ```python
  from bs4 import BeautifulSoup

  html = '''
  <html>
    <head><title>示例页面</title></head>
    <body>
      <h1>标题</h1>
      <p class="content">内容1</p>
      <p class="content">内容2</p>
    </body>
  </html>
  '''

  soup = BeautifulSoup(html, 'html.parser')
  print(soup.title.text)  # 输出：示例页面
  print(soup.h1.text)     # 输出：标题
  print([p.text for p in soup.find_all('p', class_='content')])  # 输出：['内容1', '内容2']
  ```

- 常用方法：
  - `soup.find(tag, attrs)`：查找第一个指定标签
  - `soup.find_all(tag, attrs)`：查找所有指定标签
  - `tag.text`：获取标签内的文本内容
  - `tag['属性名']`：获取标签属性值

- 注意事项：
  - BeautifulSoup 只负责解析，不负责网络请求，通常与 requests 联合使用。
  - 支持多种解析器，推荐使用 `'html.parser'`（内置）、`'lxml'`（需额外

### 爬虫实践
许多网站不希望自己的的服务器资源被爬，你需要模拟人类。
````py
import requests
from fake_useragent import UserAgent
resp = requests.get(
    "http://example.com"
)
````
cookie有时效性，复制来的COokie每隔一段时间需要重新复制，爬虫可能会被打断。
虽然没登陆，许多网站还是需要用到cookie
cookie是高度私密的，一旦别人掌握了你的cookie，就可以完全访问你的信息

嘻嘻试试吧~

有时候不需要sleep，因为爬虫没有到服务器端。